#!/usr/bin/env python3
"""
AI æ€»ç»“ç”Ÿæˆè„šæœ¬
æ‰«æ Raindrop ç”Ÿæˆçš„ Markdown æ–‡ä»¶ï¼Œå¯¹ç¼ºå¤± AI æ€»ç»“çš„æ–‡ä»¶è°ƒç”¨æ¥å£è¡¥å……å†…å®¹ã€‚
"""

import os
import sys
import json
import time
import requests
import re
from pathlib import Path
from datetime import datetime, timedelta

class AISummarizer:
    """
    å¾—åˆ°/ç½—è¾‘å®éªŒå®¤ AI æ€»ç»“å™¨
    """
    def __init__(self, api_token: str):
        self.api_token = api_token
        self.url = "https://get-notes.luojilab.com/voicenotes/web/notes/stream"
        
    def summarize(self, target_url: str) -> tuple[str, str]:
        """
        è°ƒç”¨ AI æ¥å£ç”Ÿæˆæ€»ç»“
        è¿”å›: (title, content)
        """
        headers = {
            "Authorization": f"Bearer {self.api_token}",
            "X-Request-ID": str(int(time.time() * 1000)),
            "Content-Type": "application/json"
        }
        
        payload = {
            "attachments": [
                {
                    "size": 100,
                    "type": "link",
                    "title": "",
                    "url": target_url
                }
            ],
            "content": "",
            "entry_type": "ai",
            "note_type": "link",
            "source": "web",
            "prompt_template_id": ""
        }
        
        full_title = ""
        full_content = ""
        
        try:
            print(f"   ğŸ¤– æ­£åœ¨è¯·æ±‚ AI æ€»ç»“: {target_url}")
            with requests.post(self.url, headers=headers, json=payload, stream=True) as response:
                if response.status_code != 200:
                    print(f"   âš ï¸ AI è¯·æ±‚å¤±è´¥: {response.status_code} - {response.text}")
                    return "", ""
                
                for line in response.iter_lines():
                    if not line:
                        continue
                        
                    decoded_line = line.decode('utf-8')
                    if decoded_line.startswith("data: "):
                        json_str = decoded_line[6:] # Remove 'data: ' prefix
                        try:
                            # æŸäº›å¿ƒè·³åŒ…å¯èƒ½æ˜¯ç©ºæˆ–è€…åªåŒ…å« id
                            if not json_str.strip() or json_str.strip() == "[DONE]":
                                continue
                                
                            data_obj = json.loads(json_str)
                            # Parse inner data
                            if "data" in data_obj and isinstance(data_obj["data"], dict) and "msg" in data_obj["data"]:
                                inner_msg_str = data_obj["data"]["msg"]
                                try:
                                    inner_msg = json.loads(inner_msg_str)
                                    
                                    # Accumulate title
                                    if "summary_title" in inner_msg:
                                        full_title += inner_msg["summary_title"]
                                        
                                    # Accumulate content
                                    if "content" in inner_msg:
                                        full_content += inner_msg["content"]
                                except (json.JSONDecodeError, TypeError):
                                    pass # Ignore non-json inner msg
                        except json.JSONDecodeError:
                            pass
                            
            return full_title, full_content
            
        except Exception as e:
            print(f"   âš ï¸ AI å¤„ç†å¼‚å¸¸: {e}")
            return "", ""


class AITagger:
    """
    æ™ºè°± AI æ ‡ç­¾ç”Ÿæˆå™¨
    """
    def __init__(self, api_key: str):
        self.api_key = api_key
        self.url = "https://open.bigmodel.cn/api/paas/v4/chat/completions"
        self.model = "glm-4.7-flash"

    def generate_tags(self, content: str) -> list[str]:
        """
        æ ¹æ®å†…å®¹ç”Ÿæˆæ ‡ç­¾
        """
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        prompt = """You are a bot in a read-it-later app and your responsibility is to help with automatic tagging.
Please analyze the text provided below and suggest relevant tags that describe its key themes, topics, and main ideas. The rules are:
- Aim for a variety of tags, including broad categories, specific keywords, and potential sub-genres.
- The tags language must be in chinese.
- If it's a famous website you may also include a tag for the website. If the tag is not generic enough, don't include it.
- The content can include text for cookie consent and privacy policy, ignore those while tagging.
- Aim for 3-5 tags.
- If there are no good tags, leave the array empty.

CONTENT START HERE
{content}
CONTENT END HERE

You must respond in JSON with the key "tags" and the value is an array of string tags.
"""
        
        payload = {
            "model": self.model,
            "messages": [
                {
                    "role": "system",
                    "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ã€‚"
                },
                {
                    "role": "user",
                    "content": prompt.replace("{content}", content)
                }
            ],
            "stream": False,
            "temperature": 0.1
        }

        try:
            print(f"   ğŸ·ï¸ æ­£åœ¨è¯·æ±‚ AI æ ‡ç­¾...")
            response = requests.post(self.url, headers=headers, json=payload, timeout=30)
            if response.status_code != 200:
                print(f"   âš ï¸ æ ‡ç­¾è¯·æ±‚å¤±è´¥: {response.status_code} - {response.text}")
                return []
                
            data = response.json()
            if "choices" in data and len(data["choices"]) > 0:
                content_str = data["choices"][0]["message"]["content"]
                # å°è¯•æå– JSON
                try:
                    # æŸäº›æƒ…å†µä¸‹ AI å¯èƒ½è¿”å› ```json ... ``` åŒ…è£¹
                    json_match = re.search(r'\{.*\}', content_str, re.DOTALL)
                    if json_match:
                        json_content = json_match.group(0)
                        tags_obj = json.loads(json_content)
                        return tags_obj.get("tags", [])
                except Exception as e:
                    print(f"   âš ï¸ æ ‡ç­¾è§£æå¤±è´¥: {e}")
                    
            return []
            
        except Exception as e:
            print(f"   âš ï¸ æ ‡ç­¾å¤„ç†å¼‚å¸¸: {e}")
            return []


def extract_url_from_file(file_path: Path) -> str:
    """
    ä» Markdown æ–‡ä»¶ä¸­æå– URL
    """
    try:
        content = file_path.read_text(encoding='utf-8')
        
        # 1. å°è¯•ä» FrontMatter æå– url: https://...
        match = re.search(r'^url:\s*(.+)$', content, re.MULTILINE)
        if match:
            return match.group(1).strip()
            
        # 2. å°è¯•ä»æ­£æ–‡æå– ğŸ”— [url](https://...)
        match = re.search(r'ğŸ”— \[(.*?)\]\((http.*?)\)', content)
        if match:
            return match.group(2).strip()
            
    except Exception as e:
        print(f"   è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
    
    return ""

def has_ai_summary(file_path: Path) -> bool:
    """
    æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²ç»åŒ…å« AI æ€»ç»“
    """
    try:
        content = file_path.read_text(encoding='utf-8')
        return "## ğŸ¤– AI æ·±åº¦æ€»ç»“" in content
    except:
        return False

def process_files(output_dir: str = '30_Resources/Raindrop', days: int = 3):
    """
    æ‰«æå¹¶å¤„ç†æ–‡ä»¶
    """
    dedao_token = os.getenv('DEDAO_API_TOKEN')
    zhipu_key = os.getenv('ZHIPU_API_KEY')
    
    if not dedao_token:
        print("âŒ æœªæ‰¾åˆ° DEDAO_API_TOKENï¼Œè·³è¿‡ AI æ€»ç»“æ­¥éª¤")
        return

    ai_summarizer = AISummarizer(dedao_token)
    
    ai_tagger = None
    if zhipu_key:
        ai_tagger = AITagger(zhipu_key)
        print("âœ¨ å·²å¯ç”¨ AI è‡ªåŠ¨æ ‡ç­¾ (Zhipu)")
    
    directory = Path(output_dir)
    
    if not directory.exists():
        print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {directory}")
        return

    print(f"ğŸ” å¼€å§‹æ‰«æç›®å½•: {directory}")
    print(f"   å¤„ç†æœ€è¿‘ {days} å¤©ä¿®æ”¹çš„æ–‡ä»¶")

    # è®¡ç®—æ—¶é—´é˜ˆå€¼
    cutoff_time = datetime.now() - timedelta(days=days)
    
    count = 0
    processed = 0
    
    for file_path in directory.glob('*.md'):
        # è¿‡æ»¤ä¿®æ”¹æ—¶é—´
        mtime = datetime.fromtimestamp(file_path.stat().st_mtime)
        if mtime < cutoff_time:
            continue
            
        count += 1
        
        # æ£€æŸ¥æ˜¯å¦å·²æœ‰æ€»ç»“
        if has_ai_summary(file_path):
            continue
            
        # æå– URL
        url = extract_url_from_file(file_path)
        if not url:
            print(f"â© è·³è¿‡ (æ— URL): {file_path.name}")
            continue
            
        print(f"ğŸ‘‰ å¤„ç†: {file_path.name}")
        
def inject_tags_into_frontmatter(content: str, tags: list[str]) -> str:
    """
    å°†æ ‡ç­¾æ’å…¥åˆ° FrontMatter ä¸­
    """
    if not tags:
        return content
        
    # Check for FrontMatter
    fm_match = re.match(r'^---\n(.*?)\n---', content, re.DOTALL)
    if not fm_match:
        # No FM, create one
        fm = "---\n"
        fm += "tags:\n"
        for tag in tags:
            fm += f"  - {tag}\n"
        fm += "---\n\n"
        return fm + content
        
    fm_content = fm_match.group(1)
    
    # Check if tags key exists
    if re.search(r'^tags:', fm_content, re.MULTILINE):
        # Insert after "tags:"
        # We assume standard yaml formatting created by our own script: "tags:\n"
        new_tags_str = ""
        for tag in tags:
            new_tags_str += f"  - {tag}\n"
            
        # Replace "tags:\n" with "tags:\n  - tag1\n..."
        # Use a safe replacement that looks for the line ending
        new_fm_content = re.sub(r'(^tags:\s*\n)', rf'\1{new_tags_str}', fm_content, flags=re.MULTILINE)
        
        # If the regex didn't match (maybe inline tags: []), fallback to appending
        if new_fm_content == fm_content:
             # Try to match tags: without newline
             pass 
    else:
        # Add tags key to end of FM
        new_tags_block = "\ntags:\n"
        for tag in tags:
            new_tags_block += f"  - {tag}\n"
        new_fm_content = fm_content + new_tags_block

    return content.replace(fm_match.group(1), new_fm_content)


def process_files(output_dir: str = '30_Resources/Raindrop', days: int = 3):
    """
    æ‰«æå¹¶å¤„ç†æ–‡ä»¶
    ä¼˜å…ˆè¯»å– new_files_list.txtï¼Œå¦‚æœä¸å­˜åœ¨åˆ™æ‰«æç›®å½•
    """
    dedao_token = os.getenv('DEDAO_API_TOKEN')
    zhipu_key = os.getenv('ZHIPU_API_KEY')
    
    if not dedao_token:
        print("âŒ æœªæ‰¾åˆ° DEDAO_API_TOKENï¼Œè·³è¿‡ AI æ€»ç»“æ­¥éª¤")
        return

    ai_summarizer = AISummarizer(dedao_token)
    
    ai_tagger = None
    if zhipu_key:
        ai_tagger = AITagger(zhipu_key)
        print("âœ¨ å·²å¯ç”¨ AI è‡ªåŠ¨æ ‡ç­¾ (Zhipu)")
    
    directory = Path(output_dir)
    if not directory.exists():
        print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {directory}")
        return

    # Check for report file
    workspace = os.getenv('GITHUB_WORKSPACE', '.')
    report_file = Path(workspace) / 'new_files_list.txt'
    target_files = []
    
    if report_file.exists():
        print(f"ï¿½ å‘ç°åŒæ­¥åˆ—è¡¨: {report_file}")
        try:
            with open(report_file, 'r', encoding='utf-8') as f:
                for line in f:
                    filename = line.strip()
                    if filename:
                        file_path = directory / filename
                        if file_path.exists():
                            target_files.append(file_path)
        except Exception as e:
            print(f"âš ï¸ è¯»å–åˆ—è¡¨å¤±è´¥: {e}")
    else:
        print(f"ğŸ” åŒæ­¥åˆ—è¡¨ä¸å­˜åœ¨ï¼Œè·³è¿‡ AI å¤„ç†")


    print(f"ğŸ¯ å¾…å¤„ç†æ–‡ä»¶æ•°: {len(target_files)}")
    
    count = 0
    processed = 0
    
    for file_path in target_files:
        count += 1
        
        # æ£€æŸ¥æ˜¯å¦å·²æœ‰æ€»ç»“
        if has_ai_summary(file_path):
            continue
            
        # æå– URL
        url = extract_url_from_file(file_path)
        if not url:
            print(f"â© è·³è¿‡ (æ— URL): {file_path.name}")
            continue
            
        print(f"ğŸ‘‰ å¤„ç†: {file_path.name}")
        
        # 1. è°ƒç”¨ AI æ€»ç»“
        title, content = ai_summarizer.summarize(url)
        
        if content:
            # 2. è°ƒç”¨ AI æ ‡ç­¾ (å¦‚æœæœ‰å†…å®¹)
            tags = []
            if ai_tagger:
                # ä½¿ç”¨ç”Ÿæˆçš„æ€»ç»“å†…å®¹ä½œä¸ºè¾“å…¥ï¼ŒèŠ‚çœ Token ä¸”æ›´ç²¾å‡†
                tags = ai_tagger.generate_tags(content[:2000]) # é™åˆ¶é•¿åº¦é˜²æ­¢è¶…é•¿
            
            try:
                # è¯»å–åŸæ–‡ä»¶å†…å®¹
                file_content = file_path.read_text(encoding='utf-8')
                
                # æ³¨å…¥æ ‡ç­¾åˆ° FrontMatter
                if tags:
                    file_content = inject_tags_into_frontmatter(file_content, tags)
                    print(f"   ğŸ·ï¸  æ³¨å…¥æ ‡ç­¾: {tags}")

                # è¿½åŠ æ€»ç»“å†…å®¹
                summary_block = f"\n\n## ğŸ¤– AI æ·±åº¦æ€»ç»“\n\n"
                if title:
                    summary_block += f"**{title}**\n\n"
                summary_block += f"{content}\n"
                
                # å†™å…¥æ›´æ–°åçš„å†…å®¹
                file_path.write_text(file_content + summary_block, encoding='utf-8')
                        
                print(f"   âœ… å·²æ›´æ–°æ–‡ä»¶")
                processed += 1
                # é¿å…è§¦å‘é¢‘ç‡é™åˆ¶ï¼Œç®€å•ä¼‘çœ 
                time.sleep(2)
            except Exception as e:
                print(f"   âŒ å†™å…¥å¤±è´¥: {e}")
            
        else:
            print(f"   â© è·³è¿‡ (AIæœªè¿”å›å†…å®¹)")

    print(f"\nğŸ“Š AI å¤„ç†å®Œæˆ: æ‰«æ {count} ä¸ªæ–‡ä»¶, å¤„ç† {processed} ä¸ª")

if __name__ == '__main__':
    # è·å–ç¯å¢ƒå˜é‡æˆ–ä½¿ç”¨é»˜è®¤å€¼
    output_dir = os.getenv('OUTPUT_DIR', '30_Resources') + '/Raindrop'
    process_files(output_dir)
