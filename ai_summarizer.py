#!/usr/bin/env python3
"""
AI æ€»ç»“ç”Ÿæˆè„šæœ¬
æ‰«æ Raindrop ç”Ÿæˆçš„ Markdown æ–‡ä»¶ï¼Œå¯¹ç¼ºå¤± AI æ€»ç»“çš„æ–‡ä»¶è°ƒç”¨æ¥å£è¡¥å……å†…å®¹ã€‚
"""

import os
import sys
import json
import time
import requests
import re
from pathlib import Path
from datetime import datetime, timedelta

class AISummarizer:
    """
    å¾—åˆ°/ç½—è¾‘å®éªŒå®¤ AI æ€»ç»“å™¨
    """
    def __init__(self, api_token: str):
        self.api_token = api_token
        self.url = "https://get-notes.luojilab.com/voicenotes/web/notes/stream"
        
    def summarize(self, target_url: str) -> tuple[str, str]:
        """
        è°ƒç”¨ AI æ¥å£ç”Ÿæˆæ€»ç»“
        è¿”å›: (title, content)
        """
        headers = {
            "Authorization": f"Bearer {self.api_token}",
            "X-Request-ID": str(int(time.time() * 1000)),
            "Content-Type": "application/json"
        }
        
        payload = {
            "attachments": [
                {
                    "size": 100,
                    "type": "link",
                    "title": "",
                    "url": target_url
                }
            ],
            "content": "",
            "entry_type": "ai",
            "note_type": "link",
            "source": "web",
            "prompt_template_id": ""
        }
        
        full_title = ""
        full_content = ""
        
        try:
            print(f"   ğŸ¤– æ­£åœ¨è¯·æ±‚ AI æ€»ç»“: {target_url}")
            with requests.post(self.url, headers=headers, json=payload, stream=True) as response:
                if response.status_code != 200:
                    print(f"   âš ï¸ AI è¯·æ±‚å¤±è´¥: {response.status_code} - {response.text}")
                    return "", ""
                
                for line in response.iter_lines():
                    if not line:
                        continue
                        
                    decoded_line = line.decode('utf-8')
                    if decoded_line.startswith("data: "):
                        json_str = decoded_line[6:] # Remove 'data: ' prefix
                        try:
                            # æŸäº›å¿ƒè·³åŒ…å¯èƒ½æ˜¯ç©ºæˆ–è€…åªåŒ…å« id
                            if not json_str.strip() or json_str.strip() == "[DONE]":
                                continue
                                
                            data_obj = json.loads(json_str)
                            # Parse inner data
                            if "data" in data_obj and isinstance(data_obj["data"], dict) and "msg" in data_obj["data"]:
                                inner_msg_str = data_obj["data"]["msg"]
                                try:
                                    inner_msg = json.loads(inner_msg_str)
                                    
                                    # Accumulate title
                                    if "summary_title" in inner_msg:
                                        full_title += inner_msg["summary_title"]
                                        
                                    # Accumulate content
                                    if "content" in inner_msg:
                                        full_content += inner_msg["content"]
                                except (json.JSONDecodeError, TypeError):
                                    pass # Ignore non-json inner msg
                        except json.JSONDecodeError:
                            pass
                            
            return full_title, full_content
            
        except Exception as e:
            print(f"   âš ï¸ AI å¤„ç†å¼‚å¸¸: {e}")
            return "", ""


class AITagger:
    """
    æ™ºè°± AI æ ‡ç­¾ç”Ÿæˆå™¨
    """
    def __init__(self, api_key: str):
        self.api_key = api_key
        self.url = "https://open.bigmodel.cn/api/paas/v4/chat/completions"
        self.model = "glm-4.7-flash"

    def generate_tags(self, content: str) -> list[str]:
        """
        æ ¹æ®å†…å®¹ç”Ÿæˆæ ‡ç­¾
        """
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        prompt = """You are a bot in a read-it-later app and your responsibility is to help with automatic tagging.
Please analyze the text provided below and suggest relevant tags that describe its key themes, topics, and main ideas. The rules are:
- Aim for a variety of tags, including broad categories, specific keywords, and potential sub-genres.
- The tags language must be in chinese.
- If it's a famous website you may also include a tag for the website. If the tag is not generic enough, don't include it.
- The content can include text for cookie consent and privacy policy, ignore those while tagging.
- Aim for 3-5 tags.
- If there are no good tags, leave the array empty.

CONTENT START HERE
{content}
CONTENT END HERE

You must respond in JSON with the key "tags" and the value is an array of string tags.
"""
        
        payload = {
            "model": self.model,
            "messages": [
                {
                    "role": "system",
                    "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ã€‚"
                },
                {
                    "role": "user",
                    "content": prompt.replace("{content}", content)
                }
            ],
            "stream": False,
            "temperature": 0.1
        }

        try:
            print(f"   ğŸ·ï¸ æ­£åœ¨è¯·æ±‚ AI æ ‡ç­¾...")
            response = requests.post(self.url, headers=headers, json=payload, timeout=30)
            if response.status_code != 200:
                print(f"   âš ï¸ æ ‡ç­¾è¯·æ±‚å¤±è´¥: {response.status_code} - {response.text}")
                return []
                
            data = response.json()
            if "choices" in data and len(data["choices"]) > 0:
                content_str = data["choices"][0]["message"]["content"]
                # å°è¯•æå– JSON
                try:
                    # æŸäº›æƒ…å†µä¸‹ AI å¯èƒ½è¿”å› ```json ... ``` åŒ…è£¹
                    json_match = re.search(r'\{.*\}', content_str, re.DOTALL)
                    if json_match:
                        json_content = json_match.group(0)
                        tags_obj = json.loads(json_content)
                        return tags_obj.get("tags", [])
                except Exception as e:
                    print(f"   âš ï¸ æ ‡ç­¾è§£æå¤±è´¥: {e}")
                    
            return []
            
        except Exception as e:
            print(f"   âš ï¸ æ ‡ç­¾å¤„ç†å¼‚å¸¸: {e}")
            return []


def extract_url_from_file(file_path: Path) -> str:
    """
    ä» Markdown æ–‡ä»¶ä¸­æå– URL
    """
    try:
        content = file_path.read_text(encoding='utf-8')
        
        # 1. å°è¯•ä» FrontMatter æå– url: https://...
        match = re.search(r'^url:\s*(.+)$', content, re.MULTILINE)
        if match:
            return match.group(1).strip()
            
        # 2. å°è¯•ä»æ­£æ–‡æå– ğŸ”— [url](https://...)
        match = re.search(r'ğŸ”— \[(.*?)\]\((http.*?)\)', content)
        if match:
            return match.group(2).strip()
            
    except Exception as e:
        print(f"   è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
    
    return ""

def has_ai_summary(file_path: Path) -> bool:
    """
    æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²ç»åŒ…å« AI æ€»ç»“
    """
    try:
        content = file_path.read_text(encoding='utf-8')
        return "## ğŸ¤– AI æ·±åº¦æ€»ç»“" in content
    except:
        return False

def process_files(output_dir: str = '30_Resources/Raindrop', days: int = 3):
    """
    æ‰«æå¹¶å¤„ç†æ–‡ä»¶
    """
    dedao_token = os.getenv('DEDAO_API_TOKEN')
    zhipu_key = os.getenv('ZHIPU_API_KEY')
    
    if not dedao_token:
        print("âŒ æœªæ‰¾åˆ° DEDAO_API_TOKENï¼Œè·³è¿‡ AI æ€»ç»“æ­¥éª¤")
        return

    ai_summarizer = AISummarizer(dedao_token)
    
    ai_tagger = None
    if zhipu_key:
        ai_tagger = AITagger(zhipu_key)
        print("âœ¨ å·²å¯ç”¨ AI è‡ªåŠ¨æ ‡ç­¾ (Zhipu)")
    
    directory = Path(output_dir)
    
    if not directory.exists():
        print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {directory}")
        return

    print(f"ğŸ” å¼€å§‹æ‰«æç›®å½•: {directory}")
    print(f"   å¤„ç†æœ€è¿‘ {days} å¤©ä¿®æ”¹çš„æ–‡ä»¶")

    # è®¡ç®—æ—¶é—´é˜ˆå€¼
    cutoff_time = datetime.now() - timedelta(days=days)
    
    count = 0
    processed = 0
    
    for file_path in directory.glob('*.md'):
        # è¿‡æ»¤ä¿®æ”¹æ—¶é—´
        mtime = datetime.fromtimestamp(file_path.stat().st_mtime)
        if mtime < cutoff_time:
            continue
            
        count += 1
        
        # æ£€æŸ¥æ˜¯å¦å·²æœ‰æ€»ç»“
        if has_ai_summary(file_path):
            continue
            
        # æå– URL
        url = extract_url_from_file(file_path)
        if not url:
            print(f"â© è·³è¿‡ (æ— URL): {file_path.name}")
            continue
            
        print(f"ğŸ‘‰ å¤„ç†: {file_path.name}")
        
        # 1. è°ƒç”¨ AI æ€»ç»“
        title, content = ai_summarizer.summarize(url)
        
        if content:
            # 2. è°ƒç”¨ AI æ ‡ç­¾ (å¦‚æœæœ‰å†…å®¹)
            tags = []
            if ai_tagger:
                # ä½¿ç”¨ç”Ÿæˆçš„æ€»ç»“å†…å®¹ä½œä¸ºè¾“å…¥ï¼ŒèŠ‚çœ Token ä¸”æ›´ç²¾å‡†
                tags = ai_tagger.generate_tags(content[:2000]) # é™åˆ¶é•¿åº¦é˜²æ­¢è¶…é•¿
            
            try:
                with open(file_path, 'a', encoding='utf-8') as f:
                    f.write(f"\n\n## ğŸ¤– AI æ·±åº¦æ€»ç»“\n\n")
                    if title:
                        f.write(f"**{title}**\n\n")
                    f.write(f"{content}\n")
                    
                    if tags:
                        # Obsidian æ ¼å¼: #Tag1 #Tag2
                        tag_line = ' '.join([f"#{t}" for t in tags])
                        f.write(f"\n**AI æ ‡ç­¾**: {tag_line}\n")
                        print(f"   ğŸ·ï¸  æ·»åŠ æ ‡ç­¾: {tag_line}")
                        
                print(f"   âœ… å·²è¿½åŠ æ€»ç»“")
                processed += 1
                # é¿å…è§¦å‘é¢‘ç‡é™åˆ¶ï¼Œç®€å•ä¼‘çœ 
                time.sleep(2)
            except Exception as e:
                print(f"   âŒ å†™å…¥å¤±è´¥: {e}")
        else:
            print(f"   â© è·³è¿‡ (AIæœªè¿”å›å†…å®¹)")

    print(f"\nğŸ“Š AI å¤„ç†å®Œæˆ: æ‰«æ {count} ä¸ªæ–‡ä»¶, å¤„ç† {processed} ä¸ª")

if __name__ == '__main__':
    # è·å–ç¯å¢ƒå˜é‡æˆ–ä½¿ç”¨é»˜è®¤å€¼
    output_dir = os.getenv('OUTPUT_DIR', '30_Resources') + '/Raindrop'
    process_files(output_dir)
